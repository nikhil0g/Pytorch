{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0e8876",
   "metadata": {},
   "source": [
    "# Bag-of-Words and TF-IDF representations\n",
    "### In the previous unit, we have learnt to represent text by numbers. In this unit, we'll explore some of the approaches to feeding variable-length text into a neural network to collapse the input sequence into a fixed-length vector, which can then be used in the classifier.\n",
    "\n",
    "### To begin with, let's load our AG News dataset and build the vocabulary, we we have done in the previous unit. To make things shorter, all those operations are combined into load_dataset function of the accompanying Python module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt\n",
    "#!wget -q https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/torchnlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950046ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size = \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f0a8b",
   "metadata": {},
   "source": [
    "# Bag of Words text representation\n",
    "### Because words represent meaning, sometimes we can figure out the meaning of a text by just looking at the individual words, regardless of their order in the sentence. For example, when classifying news, words like weather, snow are likely to indicate weather forecast, while words like stocks, dollar would count towards financial news.\n",
    "\n",
    "### Bag of Words (BoW) vector representation is the most commonly used traditional vector representation. Each word is linked to a vector index, vector element contains the number of occurrences of a word in a given document.\n",
    "### Note: You can also think of BoW as a sum of all one-hot-encoded vectors for individual words in the text.\n",
    "### Below is an example of how to generate a bag of word representation for a text using vectorization defined previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bow(text,bow_vocab_size=vocab_size):\n",
    "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i<bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(f\"sample text:\\n{train_dataset[0][1]}\")\n",
    "print(f\"\\nBoW vector:\\n{to_bow(train_dataset[0][1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d8980",
   "metadata": {},
   "source": [
    "### Note: Here we are using global vocab_size variable to specify default size of the vocabulary. Since often vocabulary size are pretty big, we can limit the size of the vocabulary to most frequent words. Try lowering vocab_size value and running the code below, and see how it affects the accuracy. You should expect some accuracy drop, but not dramatic, in lieu of higher performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f0c49",
   "metadata": {},
   "source": [
    "# Training BoW classifier\n",
    "### Now that we have learned how to build a Bag-of-Words representation of our text, let's train a classifier on top of it. First, we need to convert our dataset for training in such a way, that all positional vector representations are converted to bag-of-words representation. This can be achieved by passing bowify function as collate_fn parameter to standard torch DataLoader. The collate_fn gives you the ability to apply your own function to the dataset as it's loaded by the Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "# this collate function gets list of batch_size tuples, and needs to \n",
    "# return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return (\n",
    "            torch.LongTensor([t[0]-1 for t in b]),\n",
    "            torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3bbb4",
   "metadata": {},
   "source": [
    "### Now let's define a simple classifier neural network that contains one linear layer. The size of the input vector equals to vocab_size, and the output size corresponds to the number of classes (4). Because we are solving a classification task, the final activation function is LogSoftmax()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d28ca7",
   "metadata": {},
   "source": [
    "### Now we will define a standard PyTorch training loop. Because our dataset is quite large, for our teaching purpose we will train only for one epoch, and sometimes even for less than an epoch (specifying the epoch_size parameter allows us to limit training). We would also report accumulated training accuracy during training; the frequency of reporting is specified using report_freq parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,features in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = net(features)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7141a",
   "metadata": {},
   "source": [
    "### Let's see how the classifier performs on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch(net,train_loader,epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f18ee3",
   "metadata": {},
   "source": [
    "### The Bag-of-Words approach can be used in the same manner with n-gram tokenizer - only that the vocabulary size would be bigger, and thus the network would have too many parameters. In the next unit, we will see how bigram representation can be used together with embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573f23c",
   "metadata": {},
   "source": [
    "## Term Frequency / Inverse Document Frequency: TF-IDF\n",
    "### In BoW representation, word occurrences are evenly weighted, regardless of the word itself. However, it is clear that frequent words, such as 'a', 'in', 'the' etc. are much less important for the classification, than specialized terms. In fact, in most NLP tasks some words are more relevant than others.\n",
    "\n",
    "### TF-IDF stands for term frequency–inverse document frequency. It is a variation of bag of words, where instead of a binary 0/1 value indicating the appearance of a word in a document, a floating-point value is used, which is related to the frequency of word occurrence in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "df = torch.zeros(vocab_size)\n",
    "for _,line in train_dataset[:N]:\n",
    "    for i in set(encode(line)):\n",
    "        df[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb6de4",
   "metadata": {},
   "source": [
    "### Now that we have document frequencies for each word, we can define tf_idf function that will take a string, and produce TF-IDF vector. We will use to_bow defined above to calculate term frequency vector, and multiply it by inverse document frequency of the corresponding term. Remember that all tensor operations are element-wise, which allows us to implement the whole computation as a tensor formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(s):\n",
    "    bow = to_bow(s)\n",
    "    return bow*torch.log((N+1)/(df+1))\n",
    "\n",
    "print(tf_idf(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576093e4",
   "metadata": {},
   "source": [
    "### Even though TF-IDF representation calculates different weights to different words according to their importance, it is unable to correctly capture the meaning, largely because the order of words in the sentence is still not taken into account. As the famous linguist J. R. Firth said in 1935, “The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously”. We will learn in the later units how to capture contextual information from text using language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea7e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
