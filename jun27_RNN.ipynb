{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c70bd47",
   "metadata": {},
   "source": [
    "# Capture patterns with recurrent neural networks\n",
    "## Recurrent neural networks\n",
    "### In the previous module, we have been using rich semantic representations of text, and a simple linear classifier on top of the embeddings. What this architecture does is to capture aggregated meaning of words in a sentence, but it does not take into account the order of words, because aggregation operation on top of embeddings removed this information from the original text. Because these models are unable to model word ordering, they cannot solve more complex or ambiguous tasks such as text generation or question answering.\n",
    "\n",
    "### To capture the meaning of text sequence, we need to use another neural network architecture, which is called a recurrent neural network, or RNN. In RNN, we pass our sentence through the network one word vector from a news article sequence at a time, and the network produces some state, which we then pass to the network again with the next one word vector from the sequence. RNN storing a \"memory\" of the previous in the state, helps the network understand the context of the sentence to be able to predict the network word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ea07db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchinfo import summary\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba44fa2",
   "metadata": {},
   "source": [
    "## Simple RNN classifier\n",
    "### In the case of simple RNN, each recurrent unit is a simple linear network, which takes concatenated input vector and state vector, and produce a new state vector. PyTorch represents this unit with RNNCell class, and a networks of each cells - as RNN layer.\n",
    "\n",
    "### To define an RNN classifier, we will first apply an embedding layer to lower the dimensionality of input vocabulary, and then have a RNN layer on top of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc5db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = torch.nn.RNN(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x,h = self.rnn(x)\n",
    "        return self.fc(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496766c",
   "metadata": {},
   "source": [
    "### In our case, we will use padded data loader, so each batch will have a number of padded sequences of the same length. RNN layer will take the sequence of embedding tensors, and produce two outputs:\n",
    "\n",
    "### The input to the embedding layer is the word sequence or news article\n",
    "### The embedding layer output contains the vector index value in vocab for each word in the sequence\n",
    "### x is a sequence of RNN cell outputs at each step.\n",
    "###  h is a final hidden state for the last element of the sequence. Each RNN hidden layer stores the prior word in the sequence and the current as each word in the sequence is passed through the layers.\n",
    "### We then apply a fully-connected linear classifier to get the probability for number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e17de",
   "metadata": {},
   "source": [
    "### Note: RNNs are quite difficult to train, because once the RNN cells are unrolled along the sequence length, the resulting number of layers involved in back propagation is quite large. Thus we need to select small learning rate, and train the network on larger dataset to produce good results. It can take quite a long time, so using GPU is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ac787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)\n",
    "net = RNNClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffc3a2",
   "metadata": {},
   "source": [
    "### Now, let's load the test dataset to evaluate the trained RNN model. We'll be using the 4 different classes of the news categories to map the predicted output with the targeted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'class map: {classes}')\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872ce1d",
   "metadata": {},
   "source": [
    "### Before we evaluate the model, we'll extract the padded vector dataset from the dataloader. We will use the vocab.itos function to convert the numeric index to the word it matches in the vocabulary. When conversion from numeric to string happens for padded vectors, the '0' values are set to a special character 'unk'  as an unknown identifier. So, the character needs to be removed, depending on the unknown values from the padded zeros.\n",
    "\n",
    "### Finally, weâ€™ll run the model with our test dataset to verify if the expected output matched the predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (target, data) in enumerate(test_loader):\n",
    "        \n",
    "        word_lookup = [vocab.itos[w] for w in data[batch_idx]]\n",
    "        unknow_vals = {'<unk>'}\n",
    "        word_lookup = [ele for ele in word_lookup if ele not in unknow_vals]\n",
    "        print('Input text:\\n {}\\n'.format(word_lookup))\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        pred = net(data)\n",
    "        print(torch.argmax(pred[batch_idx]))\n",
    "        print(\"Actual:\\nvalue={}, class_name= {}\\n\".format(target[batch_idx], classes[target[batch_idx]]))\n",
    "        print(\"Predicted:\\nvalue={}, class_name= {}\\n\".format(pred[0].argmax(0),classes[pred[0].argmax(0)]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5788389",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM)\n",
    "### One of the main problems of classical RNNs is the so-called vanishing gradients problem. Because RNNs are trained end-to-end in one back-propagation pass, it is having hard times propagating error to the first layers of the network, and thus the network cannot learn relationships between distant tokens. The gradient helps in adjusting the weights during back-progagation to achieve better accuracy and reduce the error margin. If the weights are too small the network does not learn. Since the gradient decreases during back-propagation in RNNs, the network does not learn the initial inputs in the network. In other ways, the network \"forgets\" the earlier word inputs.\n",
    "\n",
    "### One of the ways to avoid this problem is to introduce explicit state management by using so called gates. There are two most known architectures of this kind: Long Short Term Memory (LSTM) and Gated Relay Unit (GRU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d185a9",
   "metadata": {},
   "source": [
    "###  While internal structure of LSTM cell may look complex, PyTorch hides this implementation inside the LSTMCell class, and provides a LSTM object to represent the whole LSTM layer. Thus, implementation of LSTM classifier will be pretty similar to the simple RNN which we have seen above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19906c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-0.5\n",
    "        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x,(h,c) = self.rnn(x)\n",
    "        return self.fc(h[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f61773",
   "metadata": {},
   "source": [
    "### Now let's train our network. Note that training LSTM is also quite slow, and you may not seem much raise in accuracy in the beginning of training. Also, you may need to play with lr learning rate parameter to find the learning rate that results in reasonable training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTMClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccbabe3",
   "metadata": {},
   "source": [
    "## Packed sequences\n",
    "### In our example, we had to pad all sequences in the minibatch with zero vectors. While it results in some memory waste, with RNNs it is more critical that additional RNN cells are created for the padded input items, which take part in training, yet do not carry any important input information. It would be much better to train RNN only to the actual sequence size.\n",
    "\n",
    "### To do that, a special format of padded sequence storage is introduced in PyTorch. Suppose we have input padded minibatch which looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00845e8",
   "metadata": {},
   "source": [
    "### [[1,2,3,4,5],\n",
    "### [6,7,8,0,0],\n",
    "### [9,0,0,0,0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6c628",
   "metadata": {},
   "source": [
    "### Here 0 represents padded values, and the actual length vector of input sequences is [5,3,1].\n",
    "\n",
    "### In order to effectively train RNN with padded sequence, we want to begin training first group of RNN cells with large minibatch ([1,6,9]), but then end processing of third sequence, and continue training with shorted minibatches ([2,7], [3,8]), and so on. Thus, packed sequence is represented as one vector - in our case [1,6,9,2,7,3,8,4,5], and length vector ([5,3,1]), from which we can easily reconstruct the original padded minibatch.\n",
    "\n",
    "### To produce packed sequence, we can use torch.nn.utils.rnn.pack_padded_sequence function. All recurrent layers, including RNN, LSTM and GRU, support packed sequences as input, and produce packed output, which can be decoded using torch.nn.utils.rnn.pad_packed_sequence.\n",
    "\n",
    "### To be able to produce packed sequence, we need to pass length vector to the network, and thus we need a different function to prepare minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_length(b):\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch and length sequence itself\n",
    "    len_seq = list(map(len,v))\n",
    "    l = max(len_seq)\n",
    "    return ( # tuple of three tensors - labels, padded features, length sequence\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v]),\n",
    "        torch.tensor(len_seq)\n",
    "    )\n",
    "\n",
    "train_loader_len = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=pad_length, shuffle=True)\n",
    "test_loader_len = torch.utils.data.DataLoader(test_dataset, batch_size=16, collate_fn=pad_length, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e58e32",
   "metadata": {},
   "source": [
    "### The actual network would be very similar to LSTMClassifier above, but forward pass will receive both padded minibatch and the vector of sequence lengths. After computing the embedding, we compute packed sequence, pass it to LSTM layer, and then unpack the result back.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6dac0e",
   "metadata": {},
   "source": [
    "### Note: We actually do not use unpacked result x, because we use output from the hidden layers in the following computations. Thus, we can remove the unpacking altogether from this code. The reason we place it here is for you to be able to modify this code easily, in case you should need to use network output in further computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c26db0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPackClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-0.5\n",
    "        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        pad_x = torch.nn.utils.rnn.pack_padded_sequence(x,lengths,batch_first=True,enforce_sorted=False)\n",
    "        _,(h,c) = self.rnn(pad_x)\n",
    "        return self.fc(h[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6dc425",
   "metadata": {},
   "source": [
    "### Now let's train our network with the padded sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTMPackClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch_emb(net,train_loader_len, lr=0.001,use_pack_sequence=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8577eb3",
   "metadata": {},
   "source": [
    "### Note: You may have noticed the parameter use_pack_sequence that we pass to the training function. Currently, pack_padded_sequence function requires length sequence tensor to be on CPU device, and thus training function needs to avoid moving the length sequence data to GPU when training. You can look into implementation of train_epoch_emb helper function in the torchnlp.py file located in the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for label,text,off in test_loader_len:\n",
    "        \n",
    "        text, label = text.to(device), label.to(device)\n",
    "        off = off.to('cpu')\n",
    "        print(f'off value: {off}')\n",
    "        pred = net(text, off )\n",
    "        print(f'target {label}')\n",
    "        y=torch.argmax(pred, dim=1)\n",
    "        print(f'pred: {y}')\n",
    "        print(\"Predicted:\\nvalue={}, class_name= {}\\n\".format(y[0],classes[y[0]]))\n",
    "        print(\"Target:\\nvalue={}, class_name= {}\\n\".format(label[0],classes[label[0]]))\n",
    "        break\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b2b753",
   "metadata": {},
   "source": [
    "## Bidirectional and multilayer RNNs\n",
    "### In our examples, all recurrent networks operated in one direction, from beginning of a sequence to the end. It looks natural, because it resembles the way we read and listen to speech. However, since in many practical cases we have random access to the input sequence, it might make sense to run recurrent computation in both directions. Such networks are call bidirectional RNNs, and they can be created by passing bidirectional=True parameter to RNN/LSTM/GRU constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f5693",
   "metadata": {},
   "source": [
    "### Example: self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True, bidrectional=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44686c",
   "metadata": {},
   "source": [
    "### When dealing with bidirectional network, we would need two hidden state vectors, one for each direction. PyTorch encodes those vectors as one vector of twice larger size, which is quite convenient, because you would normally pass the resulting hidden state to fully-connected linear layer, and you would just need to take this increase in size into account when creating the layer.\n",
    "\n",
    "### Recurrent network, one-directional or bidirectional, captures certain patterns within a sequence, and can store them into state vector or pass into output. As with convolutional networks, we can build another recurrent layer on top of the first one to capture higher level patterns, build from low-level patterns extracted by the first layer. This leads us to the notion of multi-layer RNN, which consists of two or more recurrent networks, where output of the previous layer is passed to the next layer as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07dd288",
   "metadata": {},
   "source": [
    "### PyTorch makes constructing such networks an easy task, because you just need to pass num_layers parameter to RNN/LSTM/GRU constructor to build several layers of recurrence automatically. This would also mean that the size of hidden/state vector would increase proportionally, and you would need to take this into account when handling the output of recurrent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d332f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
